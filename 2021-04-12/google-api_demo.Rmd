---
title: "Google API Demo"
author: "A.Chafetz"
date: "4/12/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

We work with the GSuite for a lot of our work at USAID. We can take advantage of the Google API improve the automation and collaboration of our workflows, from pushing ggplot outputs to a Google Drive folder or reading directly in a Google Sheet. 

The two main packages we use are part of the Tidyverse - `googledrive` and `googlesheets4` - and maintain by the RStudio team. The underlying package that allows for authentication is `gargle` and only the older version of the package is whitelisted for use by USAID at the momement. You will need to install an older version of both `gargle` and `googlesheets4` using the code below.

```{r, eval=FALSE}
install.packages("devtools")
install.packages("googledrive")
devtools::install_version("gargle", version = "0.5.0", repos = "http://cran.us.r-project.org")
devtools::install_version("googlesheets4", version = "0.2.0", repos = "http://cran.us.r-project.org")
```

Let's check that we have the older versions of both.

```{r}
packageVersion("gargle") == "0.5.0"
packageVersion("googlesheets4") == "0.2.0"
```

## Providing authentication

To start connecting with the Google API, you need to provide your USAID email. The very first time it will launch your internet browser and ask you to confirm approval. You can enter your email or have it prompt you to write in your email if you don't write it (safer if your code is public). To use each package you will have to authenticate with each one (and have to do this every time to start a new session).

```{r, eval=FALSE}
library(googledrive)
library(googlesheets4)

drive_auth()
gs4_auth()
```

We have a nice function to make loading your email (and other credentials into a session) as part of the `glamr` package. You can view the instructions from the vignette, `credential-management`.

```{r, eval=FALSE}
devtools::install_github("USAID-OHA-SI/glamr", build_vignettes = TRUE)
vignette("credential-management", package = "glamr") #launch the vignette
```

Basically, though you have set your email and it stored is in your Options so its ready to use.

```{r, eval=FALSE}
library(glamr)
set_email("rshah@usaid.gov") #just need to set this one time

load_secrets()
```

## Problem to solve

Now we're ready to rock and roll. For the demo we're going to try to solve a problem I had to deal with the other day. For HFR, we import any sheet from the Excel submission that has "HFR" in the sheet name. This includes hidden sheets and we realized we were importing and duplicating values for Eswatini since they had a sheet called "HFR v1" that was hidden. We needed to check to see if any of their previous submissions had these extra tabs.

## Workflow

Okay, let's load the packages we need and load our email for use with `googledrive` and `googlesheets4`.

```{r message=FALSE, warning=FALSE, results='hide'}
library(tidyverse)
library(glamr)
library(googledrive)
library(googlesheets4)
library(fs)
library(readxl)
library(lubridate)
library(glue)

load_secrets()
```

For HFR, country teams submit their submissions each period via a Google Form. So we have a Google Sheet that stores all the information entered as well as the hyperlink (and id) of the submission.

With `googledrive` and `googlesheets4` we want to specify the file or folder id which is found at the end of the url. If your provide the file name, the packages search through the thousands of files on your GDrive which takes a ton of time and expends a lot of resources. **Always,always use the Google file id!**

Let's start by looking at the Google Sheet itself that contains all the information about the submissions. We have the ID we can store as an ID, `as_id()` (or `as_sheets_id()`)

```{r}
hfr_submissions <- as_id("1gQvY1KnjreRO3jl2wzuVCKmKjUUgZDwByVK1c-bzpYI")
```

And we can open this file in our browser with `googledrive::drive_browse()`.

```{r, eval=FALSE}
drive_browse(hfr_submissions)
```

Now that we have a sense of the sheet structure, let's read it in directly to our R session using `googlesheets4::read_sheet()`.

```{r message=FALSE}
df_form <- read_sheet(hfr_submissions) 

names(df_form)
```

We only need the country and hyperlink to the file, so let's filter and limit our varaibles.

```{r}
df_swz <- df_form %>% 
  select(country = `Operating Unit/Country`, 
         period = `HFR FY and Period`,
         type = `What type of submission is this?`,
         file = `Upload your HFR file(s) here`) %>% 
  filter(country == "Eswatini")

df_swz
```

For most countries, they are only submitting one file per submission, but Eswatini uploades multiple files per submission, one for each partner (you can't see it in the preview above because it gets cut off, but there are three urls in each cell). In order to use the id from the files' hyperlinks, we have to breakout the list of hyperlinks in each cell into a data frame that has one hyperlink per row. The `tidyr` package has just the tool we need for this `separate_rows()`.

```{r}
df_swz <- separate_rows(df_swz, file, sep = ", ")

df_swz
```

Great. Now we have each of the files in their own row. We need to extract the id from the url so that we can use it for downloading. We can use a regular expression to extract this, since all follow the same pattern of coming after "id=" in the url.

```{r}
ids <- df_swz %>% 
  mutate(id = str_extract(file, "(?<=id=).*")) %>% 
  pull()

ids
```

The other piece of information we need that we didn't have from the original form, is the file name. If we saved the file to the working directory, this wouldn't be an issue; but to use the `googldrive::drive_download` and save to a particular folder, you have to provide the full path, so we need the filename. We can use the `googledrive::drive_get()` function to provide additional information about the file. We'll use a function from `purrr()` to map over each id and get the information we need and combine it back into a data frame. 

```{r}
files <- map_dfr(.x = ids,
                 .f = ~drive_get(as_id(.x))) %>% 
  mutate(name = str_remove(name, " -.*(?=\\.xlsx)")) #removing the submitter's name

files
```

If you look at these file names, you'll see there are a few duplicates from resubmissions with the same names being uploaded.

```{r}
duplicated(files$name)
```

To resolve this issue, we can add the creation time, which we can extract from nested list of drive_resources to the file name.

```{r message=FALSE, warning=FALSE}
files <- files %>% 
  mutate(created_time = purrr::map_chr(drive_resource, "createdTime") %>%
                     ymd_hms(tz = "EST"),
                created_time = created_time %>% 
                  str_remove(":[:digit:]+\\.[:digit:]+Z") %>% 
                  str_remove_all("-|:") %>%
                  str_replace(" ", "-"), 
                name_new = str_replace(name, "\\.xlsx", glue("_{created_time}\\.xlsx")))

files$name_new

duplicated(files$name_new)
```


With that, we have all the information we need to download the files - we have the id and the file name. We'll create a temporary folder to store these in and then download them to that folder (one at a time).

```{r message=FALSE, warning=FALSE}
folderpath <- dir_create(file_temp())

walk2(.x = files$id,
      .y = files$name_new,
      .f = ~drive_download(as_id(.x),
                           file.path(folderpath, .y)))
```

Wonderful so we have all the Eswatini Excel submissions on our local computer. We can now check if any additional submissions had more than one HFR labeled tab. We'll use `readxl::excel_sheets()` to provide us with the list of tab names and filter out anything that won't be imported (ie anything without HFR in the name.)

```{r}
local_files <- list.files(folderpath, full.names = TRUE)

local_files %>% 
  set_names() %>% #names the vector (1,2,3...) to be the name of the file
  map_df(~ excel_sheets(.x) %>% as_tibble(),
         .id = "file") %>% 
  mutate(file = basename(file),
         value = str_trim(value)) %>% 
  filter(str_detect(value, "HFR")) %>% 
  group_by(file) %>% 
  mutate(file_id = cur_group_id(), .before = 1) %>%
  mutate(extra = value != "HFR") %>% 
  ungroup() %>% 
  rename(sheet = value) %>% 
  prinf()
```

We can now see that there was only 1 file (`file_id` = 6) that has two tabs being read in. Phew.

We could done all this manually - copying and pasting each of the 18 urls into our browser from the form, downloading them, opening each, and check each one for hidden tabs. But this workflow takes a bit to think through the first time you do it, but you have the script if you need to repeat this agian.
